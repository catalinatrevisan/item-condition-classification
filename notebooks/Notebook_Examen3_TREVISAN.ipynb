{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CWpbDC06Jkvy"
      },
      "outputs": [],
      "source": [
        "# Librerías y Configuración\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_JagwGzBJne4"
      },
      "outputs": [],
      "source": [
        "# Configuraciones generales: número de folds, semilla y opciones de validación\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "N_FOLDS = 10\n",
        "USE_ADVERSARIAL_VALIDATION = True\n",
        "USE_PSEUDO_LABELING = True\n",
        "PSEUDO_THRESHOLD = 0.95  # Solo pseudo-labels con alta confianza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcKhKg6yJnbt",
        "outputId": "e5ab9534-f06b-4e9e-8399-d1cb43c45642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando datos...\n",
            "Train shape: (70000, 45)\n",
            "Test shape: (30000, 44)\n"
          ]
        }
      ],
      "source": [
        "# Carga de Datos\n",
        "\n",
        "def load_jsonlines(file_path):\n",
        "    # Lee los archivos JSONLines (un objeto JSON por línea) y los convierte en DataFrame.\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "print(\"Cargando datos...\")\n",
        "train = load_jsonlines('archivos-analisis-predictivo-2025q2/train_data.jsonlines')\n",
        "test = load_jsonlines('archivos-analisis-predictivo-2025q2/test_data.jsonlines')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJD711bEJnPH",
        "outputId": "7a340cf0-e3a0-4dde-e480-7d587ac6dc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adversarial Validation AUC: 0.63183\n",
            "Train y Test son similares (AUC < 0.7)\n",
            "\n",
            "Samples de train más parecidos al test: 0\n"
          ]
        }
      ],
      "source": [
        "# Validación Adversarial (Detectar diferencias train/test)\n",
        "\n",
        "if USE_ADVERSARIAL_VALIDATION:\n",
        "    # Combinar train y test para detectar diferencias entre ambos\n",
        "    train_adv = train.copy()\n",
        "    test_adv = test.copy()\n",
        "    train_adv['is_test'] = 0\n",
        "    test_adv['is_test'] = 1\n",
        "    combined = pd.concat([train_adv, test_adv], ignore_index=True)\n",
        "\n",
        "    # Generar features numéricas y categóricas simples\n",
        "    adv_features = []\n",
        "\n",
        "    for col in ['price', 'base_price', 'initial_quantity', 'sold_quantity', 'available_quantity']:\n",
        "        combined[f'{col}_log'] = np.log1p(combined[col])\n",
        "        adv_features.append(f'{col}_log') # Se aplican logs para normalizar distribuciones\n",
        "\n",
        "    for col in ['site_id', 'category_id', 'listing_type_id', 'buying_mode']:\n",
        "        le = LabelEncoder()\n",
        "        combined[col] = le.fit_transform(combined[col].fillna('missing'))\n",
        "        adv_features.append(col)\n",
        "\n",
        "    # Features textuales básicas\n",
        "    combined['title_length'] = combined['title'].str.len()\n",
        "    combined['title_words'] = combined['title'].str.split().str.len()\n",
        "    adv_features.extend(['title_length', 'title_words'])\n",
        "\n",
        "    # Entrenar un modelo simple para predecir si una fila es del test\n",
        "    X_adv = combined[adv_features].fillna(-999)\n",
        "    y_adv = combined['is_test']\n",
        "\n",
        "    lgb_adv = lgb.LGBMClassifier(n_estimators=100, random_state=RANDOM_SEED, verbose=-1)\n",
        "    lgb_adv.fit(X_adv, y_adv)\n",
        "\n",
        "    # AUC mide qué tan distintas son las distribuciones train/test\n",
        "    adv_score = roc_auc_score(y_adv, lgb_adv.predict_proba(X_adv)[:, 1])\n",
        "    print(f\"Adversarial Validation AUC: {adv_score:.5f}\")\n",
        "\n",
        "    if adv_score > 0.7:\n",
        "        print(\"WARNING: Train y Test son diferentes (riesgo de overfitting)\")\n",
        "    else:\n",
        "        print(\"Train y Test son similares (AUC < 0.7)\")\n",
        "\n",
        "    # Identificar samples más parecidos al test set\n",
        "    train_similarity = lgb_adv.predict_proba(X_adv[:len(train)])[:, 1]\n",
        "    train['similarity_to_test'] = train_similarity\n",
        "    print(f\"\\nSamples de train más parecidos al test: {(train_similarity > 0.7).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOC5GB15Jtrb",
        "outputId": "7cff780e-d429-4091-8b1e-0c6557c07104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrayendo features...\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering Avanzado\n",
        "\n",
        "def extract_advanced_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Numéricas básicas y logarítmicas\n",
        "    df['price_log'] = np.log1p(df['price'])\n",
        "    df['base_price_log'] = np.log1p(df['base_price'])\n",
        "    df['price_diff'] = df['price'] - df['base_price']\n",
        "    df['price_ratio'] = df['price'] / (df['base_price'] + 1)\n",
        "\n",
        "    # Temporales\n",
        "    # Convertir strings a fechas y generar diferencias de días\n",
        "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
        "    df['stop_time'] = pd.to_datetime(df['stop_time'])\n",
        "    df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
        "    df['date_created'] = pd.to_datetime(df['date_created'])\n",
        "\n",
        "    df['listing_duration_days'] = (df['stop_time'] - df['start_time']).dt.days\n",
        "    df['days_since_created'] = (df['last_updated'] - df['date_created']).dt.days\n",
        "    df['start_month'] = df['start_time'].dt.month\n",
        "    df['start_year'] = df['start_time'].dt.year\n",
        "    df['start_dayofweek'] = df['start_time'].dt.dayofweek\n",
        "    df['start_hour'] = df['start_time'].dt.hour\n",
        "\n",
        "    # Ventas y stock\n",
        "    df['has_sales'] = (df['sold_quantity'] > 0).astype(int)\n",
        "    df['sales_rate'] = df['sold_quantity'] / (df['initial_quantity'] + 1)\n",
        "    df['stock_level'] = df['available_quantity'] / (df['initial_quantity'] + 1)\n",
        "\n",
        "    # Título\n",
        "    df['title_length'] = df['title'].str.len()\n",
        "    df['title_words'] = df['title'].str.split().str.len()\n",
        "    df['title_upper_ratio'] = df['title'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n",
        "    df['title_digit_ratio'] = df['title'].apply(lambda x: sum(1 for c in x if c.isdigit()) / (len(x) + 1))\n",
        "\n",
        "    # Palabras clave específicas\n",
        "    df['title_has_nuevo'] = df['title'].str.lower().str.contains('nuevo|new|0km', na=False).astype(int)\n",
        "    df['title_has_usado'] = df['title'].str.lower().str.contains('usado|used', na=False).astype(int)\n",
        "    df['title_has_garantia'] = df['title'].str.lower().str.contains('garantía|warranty', na=False).astype(int)\n",
        "    df['title_has_original'] = df['title'].str.lower().str.contains('original', na=False).astype(int)\n",
        "\n",
        "    # Target encoding\n",
        "    # Encoding de category_id por tasa de \"new\"\n",
        "    if 'condition' in df.columns:\n",
        "        category_new_rate = df.groupby('category_id')['condition'].apply(lambda x: (x == 'new').mean())\n",
        "        df['category_new_rate'] = df['category_id'].map(category_new_rate)\n",
        "\n",
        "        seller_new_rate = df.groupby('seller_id')['condition'].apply(lambda x: (x == 'new').mean())\n",
        "        df['seller_new_rate'] = df['seller_id'].map(seller_new_rate)\n",
        "\n",
        "    # Vendedor\n",
        "    df['is_official_store'] = df['official_store_id'].notna().astype(int)\n",
        "    seller_counts = df['seller_id'].value_counts()\n",
        "    df['seller_frequency'] = df['seller_id'].map(seller_counts)\n",
        "    df['seller_state'] = df['seller_address'].apply(lambda x: x.get('state', {}).get('name') if isinstance(x, dict) else None)\n",
        "\n",
        "    # Garantía\n",
        "    df['has_warranty'] = df['warranty'].notna().astype(int)\n",
        "    df['warranty_length'] = df['warranty'].fillna('').str.len()\n",
        "\n",
        "    # Envíos\n",
        "    df['shipping_free'] = df['shipping'].apply(lambda x: x.get('free_shipping', False) if isinstance(x, dict) else False).astype(int)\n",
        "    df['shipping_local_pickup'] = df['shipping'].apply(lambda x: x.get('local_pick_up', False) if isinstance(x, dict) else False).astype(int)\n",
        "\n",
        "    # Tags\n",
        "    df['tags_count'] = df['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "    df['has_good_quality_tag'] = df['tags'].apply(lambda x: 'good_quality_thumbnail' in x if isinstance(x, list) else False).astype(int)\n",
        "\n",
        "    # Imágenes\n",
        "    df['pictures_count'] = df['pictures'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "\n",
        "    # Atributos\n",
        "    df['attributes_count'] = df['attributes'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "\n",
        "    def extract_attribute(attrs, attr_id):\n",
        "        if isinstance(attrs, list):\n",
        "            for attr in attrs:\n",
        "                if attr.get('id') == attr_id:\n",
        "                    return attr.get('value_name', None)\n",
        "        return None\n",
        "\n",
        "    df['brand'] = df['attributes'].apply(lambda x: extract_attribute(x, 'BRAND'))\n",
        "    df['model'] = df['attributes'].apply(lambda x: extract_attribute(x, 'MODEL'))\n",
        "    df['item_condition'] = df['attributes'].apply(lambda x: extract_attribute(x, 'ITEM_CONDITION'))\n",
        "\n",
        "    # Booleanas\n",
        "    df['accepts_mercadopago_int'] = df['accepts_mercadopago'].astype(int)\n",
        "    df['automatic_relist_int'] = df['automatic_relist'].astype(int)\n",
        "    df['has_video'] = df['video_id'].notna().astype(int)\n",
        "\n",
        "    # Interacciones\n",
        "    df['price_x_pictures'] = df['price_log'] * df['pictures_count']\n",
        "    df['price_x_warranty'] = df['price_log'] * df['has_warranty']\n",
        "    df['official_x_free_shipping'] = df['is_official_store'] * df['shipping_free']\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Extrayendo features...\")\n",
        "train = extract_advanced_features(train)\n",
        "test = extract_advanced_features(test)\n",
        "\n",
        "# Aplicar target encoding del train al test\n",
        "if 'category_new_rate' in train.columns:\n",
        "    category_map = train.groupby('category_id')['category_new_rate'].first().to_dict()\n",
        "    test['category_new_rate'] = test['category_id'].map(category_map).fillna(train['category_new_rate'].mean())\n",
        "\n",
        "    seller_map = train.groupby('seller_id')['seller_new_rate'].first().to_dict()\n",
        "    test['seller_new_rate'] = test['seller_id'].map(seller_map).fillna(train['seller_new_rate'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOP2Epv2Jtlf",
        "outputId": "e5ae99b4-73c7-4fee-9b8a-871e73118360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando TF-IDF...\n",
            "\n",
            "Total features: 156\n"
          ]
        }
      ],
      "source": [
        "# Encoding y preparación\n",
        "\n",
        "cat_features = [\n",
        "    'site_id', 'listing_type_id', 'buying_mode', 'category_id',\n",
        "    'currency_id', 'status', 'sub_status', 'listing_source',\n",
        "    'brand', 'model', 'item_condition', 'seller_state', 'tags' # Added tags here\n",
        "]\n",
        "\n",
        "num_features = [\n",
        "    'price_log', 'base_price_log', 'price_diff', 'price_ratio',\n",
        "    'initial_quantity', 'sold_quantity', 'available_quantity',\n",
        "    'listing_duration_days', 'days_since_created',\n",
        "    'start_month', 'start_year', 'start_dayofweek', 'start_hour',\n",
        "    'has_sales', 'sales_rate', 'stock_level',\n",
        "    'title_length', 'title_words', 'title_upper_ratio', 'title_digit_ratio',\n",
        "    'title_has_nuevo', 'title_has_usado', 'title_has_garantia', 'title_has_original',\n",
        "    'is_official_store', 'seller_frequency',\n",
        "    'has_warranty', 'warranty_length',\n",
        "    'shipping_free', 'shipping_local_pickup',\n",
        "    'tags_count', 'has_good_quality_tag',\n",
        "    'pictures_count', 'attributes_count',\n",
        "    'accepts_mercadopago_int', 'automatic_relist_int', 'has_video',\n",
        "    'price_x_pictures', 'price_x_warranty', 'official_x_free_shipping',\n",
        "    'category_new_rate', 'seller_new_rate', 'seller_id'\n",
        "]\n",
        "\n",
        "# Convertir listas a strings para LabelEncoding\n",
        "for col in ['sub_status', 'tags']:\n",
        "    train[col] = train[col].apply(lambda x: ','.join(x) if isinstance(x, list) else str(x))\n",
        "    test[col] = test[col].apply(lambda x: ','.join(x) if isinstance(x, list) else str(x))\n",
        "\n",
        "# Label Encoding\n",
        "for col in cat_features:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = train[col].fillna('missing')\n",
        "    test[col] = test[col].fillna('missing')\n",
        "    all_values = pd.concat([train[col], test[col]]).unique()\n",
        "    le.fit(all_values)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "# TF-IDF sobre títulos\n",
        "# Convertir texto a representación numérica basada en frecuencia\n",
        "print(\"Generando TF-IDF...\")\n",
        "tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 3), min_df=3, max_df=0.9)\n",
        "train_tfidf = tfidf.fit_transform(train['title'].fillna(''))\n",
        "test_tfidf = tfidf.transform(test['title'].fillna(''))\n",
        "\n",
        "tfidf_features = [f'tfidf_{i}' for i in range(train_tfidf.shape[1])]\n",
        "tfidf_df_train = pd.DataFrame(train_tfidf.toarray(), columns=tfidf_features)\n",
        "tfidf_df_test = pd.DataFrame(test_tfidf.toarray(), columns=tfidf_features)\n",
        "\n",
        "train = pd.concat([train.reset_index(drop=True), tfidf_df_train], axis=1)\n",
        "test = pd.concat([test.reset_index(drop=True), tfidf_df_test], axis=1)\n",
        "\n",
        "# Features finales\n",
        "all_features = num_features + cat_features + tfidf_features\n",
        "\n",
        "X = train[all_features].fillna(-999)\n",
        "y = (train['condition'] == 'new').astype(int)\n",
        "X_test = test[all_features].fillna(-999)\n",
        "\n",
        "print(f\"\\nTotal features: {len(all_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td961PqEJtiZ",
        "outputId": "0f4c2686-8f95-4567-93cb-010cd5697512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "SEED 1/5: 42\n",
            "======================================================================\n",
            "  Fold 1/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[460]\tvalid_0's auc: 0.998288\n",
            "  Fold 2/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[296]\tvalid_0's auc: 0.998352\n",
            "  Fold 3/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[387]\tvalid_0's auc: 0.998488\n",
            "  Fold 4/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[362]\tvalid_0's auc: 0.998406\n",
            "  Fold 5/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[521]\tvalid_0's auc: 0.998694\n",
            "  Fold 6/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[443]\tvalid_0's auc: 0.998612\n",
            "  Fold 7/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[535]\tvalid_0's auc: 0.998244\n",
            "  Fold 8/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[392]\tvalid_0's auc: 0.998511\n",
            "  Fold 9/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[471]\tvalid_0's auc: 0.998486\n",
            "  Fold 10/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[299]\tvalid_0's auc: 0.998592\n",
            "  AUC: 0.99848\n",
            "\n",
            "======================================================================\n",
            "SEED 2/5: 123\n",
            "======================================================================\n",
            "  Fold 1/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[465]\tvalid_0's auc: 0.998394\n",
            "  Fold 2/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[503]\tvalid_0's auc: 0.998517\n",
            "  Fold 3/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[503]\tvalid_0's auc: 0.998629\n",
            "  Fold 4/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[428]\tvalid_0's auc: 0.998391\n",
            "  Fold 5/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[387]\tvalid_0's auc: 0.998832\n",
            "  Fold 6/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[481]\tvalid_0's auc: 0.998485\n",
            "  Fold 7/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[334]\tvalid_0's auc: 0.99848\n",
            "  Fold 8/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[310]\tvalid_0's auc: 0.998294\n",
            "  Fold 9/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[290]\tvalid_0's auc: 0.998441\n",
            "  Fold 10/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1086]\tvalid_0's auc: 0.998183\n",
            "  AUC: 0.99847\n",
            "\n",
            "======================================================================\n",
            "SEED 3/5: 456\n",
            "======================================================================\n",
            "  Fold 1/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[667]\tvalid_0's auc: 0.998462\n",
            "  Fold 2/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[590]\tvalid_0's auc: 0.998386\n",
            "  Fold 3/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[632]\tvalid_0's auc: 0.998794\n",
            "  Fold 4/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[360]\tvalid_0's auc: 0.998392\n",
            "  Fold 5/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[491]\tvalid_0's auc: 0.998642\n",
            "  Fold 6/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[564]\tvalid_0's auc: 0.998013\n",
            "  Fold 7/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[375]\tvalid_0's auc: 0.998516\n",
            "  Fold 8/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[363]\tvalid_0's auc: 0.998544\n",
            "  Fold 9/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[437]\tvalid_0's auc: 0.998166\n",
            "  Fold 10/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[350]\tvalid_0's auc: 0.998782\n",
            "  AUC: 0.99849\n",
            "\n",
            "======================================================================\n",
            "SEED 4/5: 789\n",
            "======================================================================\n",
            "  Fold 1/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[408]\tvalid_0's auc: 0.99846\n",
            "  Fold 2/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[316]\tvalid_0's auc: 0.998393\n",
            "  Fold 3/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[398]\tvalid_0's auc: 0.998542\n",
            "  Fold 4/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[490]\tvalid_0's auc: 0.998414\n",
            "  Fold 5/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[349]\tvalid_0's auc: 0.998817\n",
            "  Fold 6/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[442]\tvalid_0's auc: 0.998554\n",
            "  Fold 7/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[549]\tvalid_0's auc: 0.998676\n",
            "  Fold 8/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[301]\tvalid_0's auc: 0.998359\n",
            "  Fold 9/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[635]\tvalid_0's auc: 0.998261\n",
            "  Fold 10/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[505]\tvalid_0's auc: 0.998094\n",
            "  AUC: 0.99848\n",
            "\n",
            "======================================================================\n",
            "SEED 5/5: 2024\n",
            "======================================================================\n",
            "  Fold 1/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[355]\tvalid_0's auc: 0.99803\n",
            "  Fold 2/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[399]\tvalid_0's auc: 0.998465\n",
            "  Fold 3/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[396]\tvalid_0's auc: 0.998575\n",
            "  Fold 4/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[350]\tvalid_0's auc: 0.998485\n",
            "  Fold 5/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[594]\tvalid_0's auc: 0.998558\n",
            "  Fold 6/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[320]\tvalid_0's auc: 0.99863\n",
            "  Fold 7/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[274]\tvalid_0's auc: 0.998404\n",
            "  Fold 8/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[403]\tvalid_0's auc: 0.998572\n",
            "  Fold 9/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[507]\tvalid_0's auc: 0.998474\n",
            "  Fold 10/10... Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[395]\tvalid_0's auc: 0.998323\n",
            "  AUC: 0.99847\n",
            "\n",
            "======================================================================\n",
            "RESULTADO FINAL CON MÚLTIPLES SEEDS\n",
            "======================================================================\n",
            "AUC Final OOF: 0.99849\n"
          ]
        }
      ],
      "source": [
        "# Entrenamiento con mpultiples seeds\n",
        "\n",
        "# Defino 5 seeds distintas para repetir el proceso y asegurar estabilidad.\n",
        "# Entrenar con distintas semillas reduce la varianza de los resultados y confirma\n",
        "# que el modelo no depende del azar.\n",
        "SEEDS = [42, 123, 456, 789, 2024]  # 5 seeds diferentes\n",
        "\n",
        "all_oof_preds = []    # guardará las predicciones out-of-fold (validación) por seed\n",
        "all_test_preds = []   # guardará las predicciones sobre test por seed\n",
        "\n",
        "# Recorro cada semilla y repito el entrenamiento completo\n",
        "for seed_idx, seed in enumerate(SEEDS):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"SEED {seed_idx + 1}/{len(SEEDS)}: {seed}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Uso validación cruzada estratificada (mantiene la proporción de clases)\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
        "\n",
        "    # Inicializo vectores vacíos para guardar resultados de cada modelo\n",
        "    oof_lgb = np.zeros(len(X))\n",
        "    oof_xgb = np.zeros(len(X))\n",
        "    oof_cat = np.zeros(len(X))\n",
        "\n",
        "    predictions_lgb = np.zeros(len(X_test))\n",
        "    predictions_xgb = np.zeros(len(X_test))\n",
        "    predictions_cat = np.zeros(len(X_test))\n",
        "\n",
        "    # Entrenamiento con validación cruzada\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"  Fold {fold}/{N_FOLDS}...\", end=' ')\n",
        "\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # MODELO 1: LightGBM\n",
        "        lgb_params = {\n",
        "            'objective': 'binary',    # problema de clasificación binaria\n",
        "            'metric': 'auc',          # metrica AUC para medir el rendimiento\n",
        "            'learning_rate': 0.02,    # tasa de aprendizaje baja (modelo más estable)\n",
        "            'num_leaves': 45,\n",
        "            'max_depth': 9,\n",
        "            'min_child_samples': 30,\n",
        "            'subsample': 0.85,\n",
        "            'colsample_bytree': 0.85,\n",
        "            'reg_alpha': 1.0,\n",
        "            'reg_lambda': 1.0,\n",
        "            'random_state': seed,\n",
        "            'verbose': -1\n",
        "        }\n",
        "\n",
        "        # Creo datasets para LightGBM\n",
        "        lgb_train = lgb.Dataset(X_tr, y_tr)\n",
        "        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
        "\n",
        "        # Entreno el modelo con early stopping → se detiene si no mejora por 100 rondas\n",
        "        model_lgb = lgb.train(lgb_params,\n",
        "                            lgb_train,\n",
        "                            num_boost_round=5000,  # máximo de iteraciones (usualmente corta antes)\n",
        "                            valid_sets=[lgb_val],\n",
        "                            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
        "        )\n",
        "\n",
        "        # Guardo predicciones sobre validación y test\n",
        "        oof_lgb[val_idx] = model_lgb.predict(X_val)\n",
        "        predictions_lgb += model_lgb.predict(X_test) / N_FOLDS\n",
        "\n",
        "        # MODELO 2: XGBoost\n",
        "        model_xgb = xgb.XGBClassifier(\n",
        "            learning_rate=0.02, max_depth=9, min_child_weight=5,\n",
        "            subsample=0.85, colsample_bytree=0.85,\n",
        "            reg_alpha=1.0, reg_lambda=1.0,\n",
        "            n_estimators=2500, random_state=seed,\n",
        "            tree_method='hist', eval_metric='auc'\n",
        "        )\n",
        "        model_xgb.fit(X_tr, y_tr)\n",
        "\n",
        "        oof_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n",
        "        predictions_xgb += model_xgb.predict_proba(X_test)[:, 1] / N_FOLDS\n",
        "\n",
        "        # MODELO 3: CatBoost\n",
        "        model_cat = CatBoostClassifier(\n",
        "            iterations=2500, learning_rate=0.02, depth=9,\n",
        "            l2_leaf_reg=7, random_state=seed,\n",
        "            verbose=0, early_stopping_rounds=150\n",
        "        )\n",
        "        model_cat.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True)\n",
        "\n",
        "        oof_cat[val_idx] = model_cat.predict_proba(X_val)[:, 1]\n",
        "        predictions_cat += model_cat.predict_proba(X_test)[:, 1] / N_FOLDS\n",
        "\n",
        "    # Ensemble de los 3 modelos\n",
        "    # Combinar los 3 modelos con pesos similares\n",
        "    # La idea es promediar sus errores y obtener una predicción más estable.\n",
        "    oof_ensemble = 0.35 * oof_lgb + 0.35 * oof_xgb + 0.30 * oof_cat\n",
        "    pred_ensemble = 0.35 * predictions_lgb + 0.35 * predictions_xgb + 0.30 * predictions_cat\n",
        "\n",
        "    all_oof_preds.append(oof_ensemble)\n",
        "    all_test_preds.append(pred_ensemble)\n",
        "\n",
        "    # Calcular el AUC de la combinación\n",
        "    print(f\"  AUC: {roc_auc_score(y, oof_ensemble):.5f}\")\n",
        "\n",
        "# Promedio final entre todas las seeds\n",
        "final_oof = np.mean(all_oof_preds, axis=0)\n",
        "final_test_pred = np.mean(all_test_preds, axis=0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTADO FINAL CON MÚLTIPLES SEEDS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"AUC Final OOF: {roc_auc_score(y, final_oof):.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xZJ2vq_aKGb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples confiables (>0.95 o <0.050000000000000044): 23042/30000\n",
            "Entrenando con 93042 samples (original + pseudo)...\n",
            "\n",
            "Pseudo-labeling aplicado\n"
          ]
        }
      ],
      "source": [
        "# Pseudo-Labeling\n",
        "\n",
        "if USE_PSEUDO_LABELING:\n",
        "\n",
        "    # Seleccionar predicciones del test más confiables\n",
        "    confident_mask = (final_test_pred > PSEUDO_THRESHOLD) | (final_test_pred < (1 - PSEUDO_THRESHOLD))\n",
        "    n_confident = confident_mask.sum()\n",
        "\n",
        "    print(f\"Samples confiables (>{PSEUDO_THRESHOLD} o <{1-PSEUDO_THRESHOLD}): {n_confident}/{len(X_test)}\")\n",
        "\n",
        "    if n_confident > 0:\n",
        "        # Agrego esas muestras al train con las etiquetas estimadas\n",
        "        X_pseudo = X_test[confident_mask]\n",
        "        y_pseudo = (final_test_pred[confident_mask] > 0.5).astype(int)\n",
        "\n",
        "        X_extended = pd.concat([X, X_pseudo], ignore_index=True)\n",
        "        y_extended = pd.concat([y, pd.Series(y_pseudo)], ignore_index=True)\n",
        "\n",
        "        print(f\"Entrenando con {len(X_extended)} samples (original + pseudo)...\")\n",
        "\n",
        "        # Re-entrenar un modelo LightGBM final con los datos extendidos\n",
        "        model_final = lgb.LGBMClassifier(\n",
        "            n_estimators=1500, learning_rate=0.02,\n",
        "            num_leaves=45, max_depth=9,\n",
        "            subsample=0.85, colsample_bytree=0.85,\n",
        "            reg_alpha=1.0, reg_lambda=1.0,\n",
        "            random_state=RANDOM_SEED, verbose=-1\n",
        "        )\n",
        "        model_final.fit(X_extended, y_extended)\n",
        "\n",
        "        final_test_pred = model_final.predict_proba(X_test)[:, 1]\n",
        "        print(\"\\nPseudo-labeling aplicado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_NSVneTMKIYm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy OOF: 0.97980\n",
            "Mejor threshold: 0.4836\n"
          ]
        }
      ],
      "source": [
        "# Optimización del Threshold\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Busco el threshold que maximiza el F1-score\n",
        "precision, recall, thresholds = precision_recall_curve(y, final_oof)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "# Aplico el threshold óptimo a las predicciones\n",
        "oof_pred_binary = (final_oof > best_threshold).astype(int)\n",
        "print(f\"Accuracy OOF: {accuracy_score(y, oof_pred_binary):.5f}\")\n",
        "print(f\"Mejor threshold: {best_threshold:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xki-_ZhKJiHD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo de submission generado correctamente.\n"
          ]
        }
      ],
      "source": [
        "# Generar archivo final de submission\n",
        "submission = pd.DataFrame({\n",
        "    'ID': pd.Series(range(1,len(test)+1)),\n",
        "    'condition': (final_test_pred > best_threshold).astype(int)\n",
        "})\n",
        "\n",
        "# Mapear las etiquetas binarias a los nombres del dataset original\n",
        "submission['condition'] = submission['condition'].map({0: 'used', 1: 'new'})\n",
        "\n",
        "submission.to_csv('TREVISAN_submission3.csv', index=False)\n",
        "print(\"Archivo de submission generado correctamente.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
